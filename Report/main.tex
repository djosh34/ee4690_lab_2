\documentclass[conference,a4paper,flushend]{cs-techrep}

\usepackage{todonotes}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\addbibresource{ref.bib}


\title{Hardware Implementation of MNIST digit recognizer using Binary Neural Networks


}

\author{
\IEEEauthorblockN{Joshua Azimullah}
\IEEEauthorblockA{5054354\\
j.r.azimullah@tudelft.nl}
\and
\IEEEauthorblockN{Pieter Becking}
\IEEEauthorblockA{4685377\\
PBecking@tudelft.nl}
\and
\IEEEauthorblockN{Christian van den Berg}
\IEEEauthorblockA{00000000\\
email@example.com}
\and
\IEEEauthorblockN{Ioannis Karydis}
\IEEEauthorblockA{5954460\\
ikarydis@tudelft.nl}
}

\begin{document}
\maketitle


\begin{abstract}
\end{abstract}

\todo{general todos}
\todo{work out all todos}
\todo{Each section in the beginning describes what it will say}
\todo{Connecting signal words?}


\section{Introduction}

\subsection{Basic Theory of Binary Neural Networks}


\todo{rewrite this academically}
BNN are the ultimate quantization method for neural networks. There is no way of quantizing a weight to lower than 1-bit. Quantizing to 1-bit opens up a unique new way of computation, since instead of any other number of bits representation, 1-bit multiplicaiton with 1-bit produces only 1 bit. Therefore a multiplicaiton can be modelled as a truth table. This opens the possibility of many hardware efficiency improvements over traditional quantized and floating-point neural nets.


Overall, BNNs offer a promising approach to developing efficient and high-performance neural networks suitable for a wide range of practical applications.


Binary Neural Networks (BNNs) represent a class of neural networks where weights and activations are constrained to binary values, typically \(\{-1, 1\}\). This binarization facilitates significant computational efficiency, particularly in hardware implementations.

% \subsection{Equivalence of Matrix Multiplication and XNOR-Popcount Operations}

In BNNs, matrix multiplication involving binary weights and activations can be effectively implemented using XNOR and popcount operations in hardware. The XNOR operation provides a binary equivalence to multiplication, while the popcount function, which counts the number of ones in the result, corresponds to the summation step in matrix multiplication. This approach is computationally efficient and well-suited to hardware accelerators.

\todo{these refs}
\todo{ref other section}
This equivalence has been detailed in various studies \cite{placeholder_ref1, placeholder_ref2}, highlighting the advantages of BNNs for high-speed and energy-efficient computations in specialized hardware.




\subsection{Contribution \& Scope of the Project}

\todo{Rewrite aim, such that our aim was to first achieve a respectible accuracy of 95\% on the MNIST dataset, and then further improve the design to optimize on area, power and minimum achievable clock speed.}

\todo{rewrite this part also include: we used mnist dataset (with reference), testset of 10.000 images. }

It covers both training and inference phases, providing a complete view of the BNN's practical use and performance.

\subsection{Outline}

\todo{written table of contents for coming sections}

\section{Overview of BNN Architecture}

\subsection{Software network architecture}
\todo{change software such that it shows how in training}
\todo{ref to why 768 and not 784}
The architecture of our final network is as follows:
\begin{itemize}
    \item The input vector consists of 768 1-bit values (\(\{-1, 1\}\)).
    \item The first layer performs matrix multiplication with a weight matrix of dimensions \(1024 \times 768\).
    \item A hard activation function is applied, producing output values in \(\{-1, 1\}\).
    \item The second layer involves matrix multiplication with a weight matrix of dimensions \(10 \times 1024\).
    \item The output vector contains 10 sums.
    \item The highest sum is selected as the prediction.
\end{itemize}

\subsection{Mathmatical hardware network architecture}
\todo{add in mathmatical hardware architecture}
\todo{add in caption to hardware saying that this is how it is mathtematically}

\subsection{Actual hardware network architecture}
\todo{add in actual hardware architecture}
\todo{image}
\todo{xnor_popcount}
\todo{matrix_2}
\todo{memory}

\section{Implementation of BNN architecture}
\subsection{Software implementation}
Our Binary Neural Networks (BNNs) are trained using PyTorch, leveraging the Adam optimizer and CrossEntropy loss function, with one-hot encoded outputs. Prior to training, the input values are rounded to \(\{-1, 1\}\), ensuring the network adapts to these binary inputs. Training is conducted in floating-point representation, with binarization applied post-training.

In the MNIST dataset, the 8-bit input values are pre-processed such that values are mapped to $1$ if $\geq 128$, and to $-1$ otherwise. The HardTanh function is used during training to nudge activations towards hard binary values. 

Parallel evaluations using NumPy, simulating the potential BNN's performance, involve rounding matrices to \(\{-1, 1\}\) to validate that inference based on the matrix multiplications using the rounded values aligns with the outcomes predicted by the PyTorch model.


\subsection{Hardware implementation}

\todo{section on hardware architecture}
\todo{ref to appendix label:xnor_maths}
\todo{csa adder tree popcount}
\todo{matrix2 counter network:}
\todo{approach 10 counters and finding highest using tree compare}

\subsection{Optimizations}
\todo{Pipelining popcount}
\todo{exploration different number of levels and bitwidths -> show in results}

\todo{subsubsection better approach using lfsr counters + 1 highest counter, which can be done since the real count doesn't need to be known. Just which one is highest}

\section{Results}

\subsection{Simulation setup}
\todo{section on testing methodology}
\todo{split into three parts}

\todo{testing individual parts using fake generated data}

\todo{subsection testing using ghdl technology for rapid iteration}

\todo{final testing whole network using all 10.000 test images}


\subsection{Demonstration}
\todo{Demonstration show accuracy from simulation}



\subsection{Design metric results}

\todo{subsection accuracy}
\todo{subsection area}
    \todo{XNOR Popcount Evaluator}
    \todo{Majority Classifier}
    \todo{memory}
\todo{subsection power}
    \todo{XNOR Popcount Evaluator}
    \todo{Majority Classifier}
    \todo{memory}
\todo{subsection timing}
    \todo{XNOR Popcount Evaluator}
    \todo{Majority Classifier}
    \todo{memory}

\subsection{General design Trade-Off}

\todo{section on hardware possible strategies}
\todo{subsubsection strategy do everything sequentially}
\todo{subsubsection strategy do everything in parallel in one tick}
\todo{subsubsection strategy mixed approach, since parallel would be huge and sequentially would be unneccesary slow for little area and power gains due to weights storage that is mandatory}

\subsection{Hardware Trade-off Results}

\subsubsection{Small section that weight memory is most of the area}
	% - [ ] full design
	% 	- [ ] note on how memory is most of the area
\subsubsection{XNOR popcount}
\todo{Explanation of results:}
\todo{0 levels and 1 levels a wide number of valid divisors of 768 were tested and shown her}
\todo{2 and 3 levels have too large exploration space thus first a reasonable setup was chosen and then for each additional test, only the part where the lowest slack was changed}
	% - [ ] xnor popcount exploration
	% 	- [ ] direct csa tree
	% 	- [ ] 1 level
	% 	- [ ] 2 levels
	% 	- [ ] 3 levels
 
\subsubsection{Matrix 2}
	% - [ ] matrix_2_design
\todo{One line of original with real adders}
\todo{One line of improved with lfsr counters}


\subsection{Accuracy Trade-Off}

\todo{why not CNN decision: would probably require somewhere a linear layer anyways, or a large number of layers vastly reducing speed}

\todo{somewhere needs to be put a better reasoning why 2 matrices are more efficient: either have 1024 counters = excesive or 1024x768 counts which is slow and area gains are low}
The initial goal of our Binary Neural Network (BNN) architecture was to achieve a high level of performance using a single matrix for computations. However, this approach did not meet the target accuracy of 95\%, necessitating exploration of alternative designs.


Subsequent experiments with two matrices in the network structure suggested that this configuration could offer a similar level of computational efficiency while potentially improving accuracy. 

\textit{Results pending. This section will present accuracy metrics for different network configurations once available.}

\todo{lower hidden size, would drop accuracy too much}
\todo{lower input size also}


To further optimize the input dimensions, we reduced the number of inputs from 784 to 768 by dropping the last 16 inputs. This simplification leverages the powers of two, specifically \(768 = 512 + 256 = 2^9 + 2^8\), which can simplify the hardware and align more efficiently with binary operations. This reduction did not impact performance.


 








\section{Conclusions}
\todo{it was less area than matrix multiplication lab-1}
\todo{timing bottleneck mostly from memory}

\todo{future work}
\todo{subsection better memory library}
\todo{subsection other problems}



\appendix

\section{Mathematical Foundations for Hardware Implementations}

\subsection{First matrix multiplication with activation}

This section demonstrates that performing matrix-vector multiplication followed by a hard activation is equivalent to using XNOR operations, popcount, and a subsequent comparison.
The matrix-vector multiplication \(\mathbf{A} \mathbf{b}\) shown in Equation \ref{eq:matrix_vector_multiplication} results in a vector \(\mathbf{c}\). Each element \(c_i\) of this vector is obtained by the dot product of the \(i\)-th row of matrix \(\mathbf{A}\) with vector \(\mathbf{b}\), as described in Equation \ref{eq:dot_product}. This can be further expanded into component-wise multiplication and summation (Equation \ref{eq:component_wise_dot}). When elements \(a_{ij}\) and \(b_j\) are binary (\(1\) or \(-1\)), we map them to \(1\) and \(0\) respectively, as shown in Equation \ref{eq:mapping}, enabling the use of XNOR for the multiplication, as described in Equation \ref{eq:xnor}.

\begin{figure}[h]
    \centering
    \begin{equation}
    \mathbf{c} = \mathbf{A} \mathbf{b}
    \label{eq:matrix_vector_multiplication}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n a_{ij} b_j
    \label{eq:dot_product}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n (a_{ij} \cdot b_j)
    \label{eq:component_wise_dot}
    \end{equation}

    \begin{equation}
    a'_{ij} = \begin{cases}
    1 & \text{if } a_{ij} = 1 \\
    0 & \text{if } a_{ij} = -1
    \end{cases}, \quad
    b'_j = \begin{cases}
    1 & \text{if } b_j = 1 \\
    0 & \text{if } b_j = -1
    \end{cases}
    \label{eq:mapping}
    \end{equation}

    \begin{equation}
    a_{ij} \cdot b_j = \text{XNOR}(a'_{ij}, b'_j)
    \label{eq:xnor}
    \end{equation}

    \caption{Matrix-vector multiplication and equivalent dot product representation.}
    \label{fig:matrix_vector_multiplication}
\end{figure}


\begin{figure}[h]
    \centering
    \begin{equation}
    c = \sum_{i=1}^n a_i b_i
    \label{eq:dot_product_sum}
    \end{equation}

    \begin{equation}
    c = \left|\{c_i \mid c_i = 1\}\right| - \left|\{c_i \mid c_i = -1\}\right|
    \label{eq:dot_product_count}
    \end{equation}

    \begin{equation}
    |\{c_i \mid c_i = -1\}| = n - |\{c_i \mid c_i = 1\}|
    \label{eq:negative_count}
    \end{equation}

    \begin{equation}
    a \cdot b = 2 \left|\{c_i \mid c_i = 1\}\right| - n
    \label{eq:dot_product_simplified}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } 2 \left|\{c_i \mid c_i = 1\}\right| \geq n \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } \left|\{c_i \mid c_i = 1\}\right| \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function_half}
    \end{equation}

    \begin{equation}
    \text{activation}(a \text{ XNOR } b) = \begin{cases}
    1 & \text{if } \text{popcount}(a \text{ XNOR } b) \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_xnor}
    \end{equation}

    \caption{Dot product calculation and activation function.}
    \label{fig:dot_product_activation}
\end{figure}

In Figure \ref{fig:dot_product_activation}, Equation \ref{eq:dot_product_sum} sums the products of the components. Equation \ref{eq:dot_product_count} translates the sum into a count of \(1\)s and \(-1\)s. The total count of \(-1\)s can be expressed as the complement of the count of \(1\)s (Equation \ref{eq:negative_count}). The dot product simplifies to Equation \ref{eq:dot_product_simplified}. The activation function (Equation \ref{eq:activation_function}) sets the output based on the threshold condition, which can also be expressed equivalently as shown in Equation \ref{eq:activation_function_half}. This is related to the XNOR operation in Equation \ref{eq:activation_xnor}, where the activation depends on the population count of the XNOR operation being greater than or equal to half the length of the vector.


\subsection{maths second matrix}
\todo{subsubsection maths is same but now the actual highest popcount needs to be found}









\end{document}

