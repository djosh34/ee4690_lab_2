% -- just a two column document
\documentclass[conference]{IEEEtran}

\usepackage[
backend=biber,
style=ieee
]{biblatex}
\addbibresource{ref.bib}

% \usepackage{todonotes}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage{etoolbox}
\usepackage{hyperref}
% \maxdeadcycles=200


% Package for better math typesetting
\usepackage{amsmath}

% Package for custom lists
\usepackage{enumitem}



% Package to count todos

% Counter for todos
\newcounter{todocount}
\setcounter{todocount}{0}

% \todo command
\newcommand{\todo}[1]{
  \stepcounter{todocount}
}

% Display todo count
\newcommand{\showtodocount}{%
  \ifthenelse{\value{todocount}=0}{%
    % No todos
    \section*{Todo List}
    No todos.
  }{%
    % Todos found
    \section*{Todo List}
    Total todos: \arabic{todocount}.
  }
}







\title{Hardware Implementation of MNIST digit recognizer using Binary Neural Networks}





\author{
\IEEEauthorblockN{Joshua Azimullah}
\IEEEauthorblockA{5054354\\
j.r.azimullah@tudelft.nl}
\and
\IEEEauthorblockN{Pieter Becking}
\IEEEauthorblockA{4685377\\
PBecking@tudelft.nl}
\and
\IEEEauthorblockN{Christian van den Berg}
\IEEEauthorblockA{00000000\\
email@example.com}
\and
\IEEEauthorblockN{Ioannis Karydis}
\IEEEauthorblockA{5954460\\
ikarydis@tudelft.nl}
}

\begin{document}
\maketitle


\begin{abstract}
\end{abstract}

\todo{general todos}
\todo{work out all todos}
\todo{Each section in the beginning describes what it will say}
\todo{Connecting signal words?}


\section{Introduction}
\label{sec:introduction}

\subsection{Basic Theory of Binary Neural Networks}


Binary Neural Networks (BNNs) are an advanced quantization method for neural networks that offers a unique approach to computation. By quantizing weights to 1-bit, BNNs streamline the multiplication process to produce more efficient results. This innovative method allows for hardware efficiency improvements when compared to traditional quantized and floating-point neural networks.

Overall, BNNs offer a promising approach to developing efficient and high-performance neural networks suitable for a wide range of practical applications. BNNs represent a class of neural networks where weights and activations are constrained to binary values, typically \(\{-1, 1\}\). This binarization facilitates significant computational efficiency, particularly in hardware implementations.

% \subsection{Equivalence of Matrix Multiplication and XNOR-Popcount Operations}

In BNNs, matrix multiplication involving binary weights and activations can be effectively implemented using XNOR and popcount operations in hardware. The XNOR operation provides a binary equivalence to multiplication, while the popcount function, which counts the number of ones in the result, corresponds to the summation step in matrix multiplication. This approach is computationally efficient and well-suited to hardware accelerators.

\todo{these refs} %DONE
\todo{ref other section} %DONE
This equivalence has been detailed in various studies \cite{courbariaux2016binarynet}, \cite{simons2019review}, highlighting the advantages of BNNs for high-speed and energy-efficient computations in specialized hardware.

\subsection{Contribution \& Scope of the Project}
\todo{Rewrite aim, such that our aim was to first achieve a respectable accuracy of 95\% on the MNIST dataset, and then further improve the design to optimize on area, power and minimum achievable clock speed.} %DONE

\todo{rewrite this part also include: we used MNIST dataset (with reference), testset of 10.000 images.} %DONE

%Our project began with the ambitious goal of achieving a significant milestone: attaining a respectable accuracy rate of 95\% on the renowned MNIST dataset, consisting of handwritten digits [1](cite the page of yann lecun??). This task was conducted using a test set comprising 10,000 images, providing a rigorous evaluation of our model's performance.

%Moving forward, our project expanded its scope to include the optimization of key parameters, namely area, power efficiency, and the minimum achievable clock speed. This optimization phase represents a critical contribution, as it delves into the intricate balance between computational efficiency and performance within our system.

%By meticulously fine-tuning these aspects, we aim to not only enhance the overall efficiency of our model but also pave the way for future advancements in machine learning hardware. This holistic approach underscores our commitment to not only achieving high accuracy but also ensuring scalability and practicality in real-world applications.

%Furthermore, our project covers both the training and inference phases, providing a comprehensive view of the BNN's practical use and performance. This ensures that our analysis extends beyond mere accuracy metrics, offering insights into the model's effectiveness across various operational contexts.


It covers both training and inference phases, providing a complete view of the BNN's practical use and performance.

\subsection{Outline}
In this report, we begin with Section \ref{sec:introduction} being an explanation of the basic theory of BNNs. We then discuss the specific contributions and scope of the project, setting the stage for the detailed exploration to follow in this outline.

Section \ref{sec:overview} provides an overview of the BNN architecture, delving into both the training architecture and the inference network architecture. This section lays the groundwork for understanding the detailed implementations and optimizations described later in the report.

In Section \ref{sec:implementation}, we delve into the implementation of the BNN architecture. This section is divided into three main parts: software implementation, hardware implementation, and various optimizations. The optimizations include techniques such as pipelining popcount and using an LFSR-based majority classifier, which are crucial for enhancing the performance of BNNs.

Section \ref{sec:results} presents the results of our work. We start with the simulation setup, followed by a demonstration of the implemented architecture. We then discuss the design metrics and trade-offs involved in the implementation. This includes hardware trade-off results, where we highlight specific aspects such as the area occupied by weight memory, XNOR popcount operations, and matrix computations. Additionally, we analyze the accuracy trade-offs, providing a comprehensive view of the performance and efficiency of our BNN implementation.

In Section \ref{sec:discussion}, we address minor accuracy differences and the potential of overfitting due to hyperparameter tuning. We justify design decisions such as using two matrices for efficiency and avoiding CNNs to maintain operational speed. Additionally, we highlight the significance of managing weight memory and propose a hybrid hardware strategy that balances speed and resource usage for fully connected BNNs.

In Section \ref{sec:conclusions}, we draw conclusions from our work, summarizing the key findings and their implications.

Finally, in Section \ref{sec:future} we highlight avenues for future research in BNN hardware implementation. Firstly, developing a specialized memory library optimized for binary operations could significantly enhance access time and energy efficiency. Secondly, experimenting with an input size of 729 for the XNOR-popcount CSA network may improve computational efficiency and reduce hardware complexity. Lastly, exploring a double precision implementation could enhance numerical stability and accuracy in certain applications, albeit with considerations for trade-offs in area, power, and performance.

The \ref{appendix}ppendix contains supplementary material, including the first matrix multiplication with activation and the mathematical details of the second matrix. References to relevant literature are provided at the end of the report to support the discussions and claims made throughout.


\todo{written table of contents for coming sections -> DONE}

\section{Overview of BNN Architecture}
\label{sec:overview}
\subsection{Training Architecture}
Section \autoref{ref:why_768} provides a rationale for choosing 768 input values over 784, demonstrating superior performance in our specific context.

The architecture of our training network is outlined as follows:
\begin{itemize}
    \item The input vector consists of 768 values, each either -1 or 1.
    \item The first layer performs a matrix multiplication using a weight matrix of size \(1024 \times 768\).
    \item A hardtanh activation function is applied, resulting in output values constrained within the range \(\{-1, 1\}\).
    \item The second layer performs a matrix multiplication with a weight matrix of size \(10 \times 1024\).
    \item The resulting output vector comprises 10 summed values.
    \item The maximum value among these 10 sums is selected as the prediction.
\end{itemize}

\subsection{Inference Network Architecture}
\label{subsec:inference_network_architecture}
The inference network shares the same mathematical structure as the training network. The primary difference lies in the activation function: the inference network employs a hard activation to constrain outputs strictly to -1 and 1.



\section{Implementation of BNN architecture}
\label{sec:implementation}
\subsection{Software implementation}
\todo{list learning rate, batch size}

Our BNNs are trained using PyTorch\cite{paszke2019pytorch}, leveraging the Adam optimizer\cite{kingma2014adam} and CrossEntropy loss function\cite{mao2023crossentropy}, with one-hot encoded outputs. Prior to training, the input values are rounded to \(\{-1, 1\}\), ensuring the network adapts to these binary inputs. Training is conducted in floating-point representation, with binarization applied post-training.

In the MNIST dataset, the 8-bit input values are pre-processed such that values are mapped to $1$ if $\geq 128$, and to $-1$ otherwise. The HardTanh function is used during training to nudge activations towards hard binary values. like in paper: \cite{yuan2023comprehensive}

Parallel evaluations using NumPy, simulating the potential BNN's performance, involve rounding matrices to \(\{-1, 1\}\) to validate that inference based on the matrix multiplications using the rounded values aligns with the outcomes predicted by the PyTorch model.


genus ??? vhdl ???



\subsection{Hardware implementation}

\autoref{fig:overview} shows the hardware architecture of the Binary Neural Network (BNN). 

The XNOR Popcount Evaluator (\autoref{fig:xnor_popcount}) performs the first matrix multiplication and activation by XNORing and popcounting the input vector with the current row of weights. It outputs a bit indicating if the sum is $\geq 384$. The Majority Classifier (\autoref{fig:majority_classifier}) conducts the second matrix multiplication, determining the output by choosing the highest sum among the columns.

In each clock cycle, a row from the first matrix (XNOR Popcount) and a column from the second matrix (Majority Classifier) are processed in parallel. The XNOR Popcount Evaluator is the bottleneck, so the second matrix operation isn't accelerated. 

On reset, both matrix\_1 and matrix\_2 counters reset to 0. These counters, as shown in \autoref{fig:overview}, select the current row and column in the respective matrix register files. The matrix\_1 counter increments each cycle until it completes the register file. 

The XNOR Popcount Evaluator takes the input vector and the current row from weights\_1, XNORs them, and popcounts the result, outputting a bit if the sum $\geq 384$. It also produces a validity bit, initially set low, which counts to a specific count $n$ using an LFSR counter, after which it is set high. This bit is set after $n$ cycles based on the optimization in Section \ref{ref:pipeline_popcount}.

The Majority Classifier activates if the validity bit is set. Upon activation, it increments the matrix\_2 counter after each cycle, resulting in a delay behind the matrix\_1 counter by $n$. No additional enable signal is needed; a reset to 0 starts the prediction process directly. The Majority Classifier also holds a count equivalent to the matrix\_2 count, and when this count equals the hidden size $1024$, the done flag is set high, signaling prediction completion.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{overview.pdf}
    \caption{Overview of the hardware architecture.}
    \label{fig:overview}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{Xnor_popcount.pdf}
    \caption{XNOR Popcount Evaluator.}
    \label{fig:xnor_popcount}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{majority_classifier.pdf}
    \caption{Majority Classifier.}
    \label{fig:majority_classifier}
\end{figure}



\subsection{Optimizations}
\subsubsection{Pipelining Popcount}
\label{ref:pipeline_popcount}

Instead of performing the popcount in a single clock cycle using a large Carry-Save Adder (CSA) network, our exploration in \autoref{ref:exploration_pipelining} determined that a pipelined approach with four registers between the adder stages is optimal. Thus, the popcount result becomes valid after $n=4$ cycles, with subsequent cycles producing new valid sums.

\subsubsection{LFSR-Based Majority Classifier}

Initially, individual counters for each digit were used, along with a tree structure to identify the highest count. However, this structure is redundant if the highest count is tracked dynamically. Each counter is compared against the current highest count. If a counter matches the highest count and is to be incremented, the highest count is updated accordingly. This update is achieved by performing a bitwise AND operation between a 10-bit vector indicating counters equal to the highest count and a 10-bit vector indicating counters to be incremented. An OR reduction of this resultant vector determines if the highest count should increase. 
Ultimately, this approach allows the additional optimization of replacing normal counters with Linear Feedback Shift Register (LFSR) counters, which provide equivalent functionality using less area \cite{xilinx_lfsr}, as is shown in Section \ref{ref:lfsr_opt}

\subsubsection{Hidden layer size optimization}
ref to hidden size optimization 


\section{Results}
\label{sec:results}
\subsection{Simulation Setup}

The initial model, developed in PyTorch, was translated into raw NumPy operations using values of 1 and -1. Upon validation, the NumPy implementation was segmented into the XNOR Popcount and Majority Classifier components.

This segmentation allowed the generation of random test data for both components, output as text files containing 0s and 1s. These text files served as inputs for testbenches to validate the functionality of each component individually.

Additionally, text files for input vectors and corresponding labels were generated using the same method. This enabled testing the entire 10,000-image MNIST \cite{mnist} testset to verify the device's operation.

Simulations were executed using GHDL \cite{ghdl} to accelerate the simulation process and facilitate rapid testing iterations.



\subsection{Demonstration}
This section provides a detailed demonstration of the simulation results. 
\autoref{fig:waveform} presents a waveform analysis demonstrating that, after the reset signal is set low, the output vector begins displaying which counters are equal to the higest count. When the done signal is high, the output equals to the expected signal, verifying the proper operation of the designed circuit.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.2\textwidth]{./testbench.png}
    \caption{Testbench output validating the images from the MNIST test set \cite{mnist}.}
    \label{fig:testbench}
\end{figure}

\autoref{fig:testbench} displays the output of the testbench, which evaluates the entire set of 10,000 images against the full MNIST test set, confirming the functionality and accuracy of the simulation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{./Waveform_picture.png}
    \caption{Waveform illustrating the comparison against the output\_row and expected\_row during the simulation of the testbench}
    \label{fig:waveform}
\end{figure}




\subsection{Accuracy Metrics}

\subsubsection{Hidden size Optimization}

since our bnn device uses a sequential method for processing the hidden layer, the hidden size can be precisely adjusted to get the best hidden size for a given accuracy.
Increasing the hidden size both increases the number of clock cycles:  clock cycles inference = hidden size + delay caused by xnor popcount evaluator. where hidden size >> delay cycles
Furthermore storing the weights as is discussed in Section \ref{ref:overall_minus_rest}, is the largest contributer for area, power and timing. and the hidden size both affects the matrix 1 and 2 linearly the number of bits needed to store.

Therefore any decrease in hidden size causes a big win in terms of performance on all metrics except accuracy.
we first did a overview hidden size exploration to get to the 95\% target first.

At first a global overview was done with the hidden sizes of powers of two: 64,128,256,512,1024,2048,4096
from here it was found that 1024 was just sufficient since an accuracy of 95\% was achieved seen in \autoref{fig:line_graph_global} <- ./Global_exploration_hidden_sizes.png

next for optimization sake every interval of 32 was tested between 512 and 1024 was tested, \autoref{fig:bar_max_achieved_accuracy <- ./Different_hidden_layer_sizes_512_1024.png} shows the max achieved accurcacy for each.

From that it was discovered that 608 as a hidden size also reached our target of 95\%. This means 





\subsection{Hardware metric results}

- text showing overall area, power, timing

\todo{Overall 1024}
  \todo{subsection area}
  \todo{subsection power}
  \todo{subsection timing}

\todo{Overall 608 after optimization}
  \todo{subsection area}
  \todo{subsection power}
  \todo{subsection timing}

\label{ref:overall_minus_rest}
\todo{Overall - rest} % register files part + counters
  \todo{subsection area}
  \todo{subsection power}
  \todo{subsection timing}

\todo{Theoritical inference performance}

\subsection{Hardware Trade-off Results}

	% - [ ] full design
	% 	- [ ] note on how memory is most of the area
\subsubsection{XNOR popcount}
\label{ref:why_768}
% To further optimize the input dimensions, we reduced the number of inputs from 784 to 768 by dropping the last 16 inputs. This simplification leverages the powers of two, specifically \(768 = 512 + 256 = 2^9 + 2^8\), which can simplify the hardware and align more efficiently with binary operations. This reduction did not impact performance.

\label{ref:exploration_pipelining}


it is often good practise to start with the simpelest model and then later optimize. first we just used a full direct csa tree for the popcount. Then to decrease the minimum slack needed, differing number of pipeline stages where added inside the adder network. 


First just the xnor popcount was synthesized using the genus \cite{genus} tool to speed up the exploration step.

// make table in latex from this : Direct CSA sum
Area	Timing	Power
	4486.888	7957	7.30E-04


// it needs to be more clear what the 16,64 and 256 mean here. so we start at the input which is the xnor output vector from \autoref{fig:xnor_popcount}. Then instead of having one csa tree to the output sum, there are multiple trees. so for 64 is meant that there are 768/64 = 12 sum registers in the in between level, that can each count up to max 64. then for each 64 bits in the xnor vector a csa tree is made to each in between sum register. then from the in between level to the output there is another csa tree adding the in between registers up to the output sum. the csa tree is not decided by us, but by the genus tool itself, since in most cases the synthesize tool knows best which type of tree to use.

Next an exploration was done using one in between level of several different sum ranges: [16,32,48,64,96,128,192,256] where for instance 16 means that the registers can store a maximum of count 16 thus requiring 4 bits.

// make table in latex from this : CSA one pipelined register, include the one with NA (they are added in later)
Area	Timing	Power		Max sum
6664.63	8318	1.30E-03		16
5739.482	8458	1.06E-03		32
5298.986	8604	9.40E-04		48
5241.796	8618	9.58E-04		64
4986.702	8528	8.79E-04		96
4937.758	8457	8.47E-04		128
4793.054	8251	8.41E-04		192
NA	NA	NA		256
NA	NA	NA		384


// now it needs to be extra clear that in the 2+ levels it is meant that for instance the 256 on level 2 means that the sum registers on that level can count up to 256. so for level 1 64 and level 2 256 it is meant that the registers in level 2 add up 256/64 registers from level 1
Next two levels were explored, however since exploring all different combinations would take very long, a compromise was reached.
the test was started by interjecting an addition level of sum register with max 256 between the 64 and output sum, where 64 was the best in terms of timing from the previous test.
then for each next test it was seen between which two levels the timing was worst, so for instance for the level 1 at 64 and level 2 at 256 the worst timing was between the input and 64, therefore for the next
test level 1 was changed to 16 etc....
each of these decisions was made solely on the timing, since area and power were mostly dominated by the memory footprint seen in \ref{ref:overall_minus_rest}

// two level explo
Area	Timing	Power	Max sum level 1	Max sum level 2	Worst timing
5364.688	8617	1.01E-03	64	256	input -> 64
6864.928	8740	1.36E-03	16	256	16 -> 256
5526.15	8750	9.84E-04	48	192	input -> 48
5960.262	8834	1.11E-03	32	192	input -> 32
5998.832	8851	1.07E-03	24	192	192 -> output sum
5899.614	8740	1.05E-03	24	384	24 -> 384


since from the table of two levels 24,192 was best with the worst timing being 192->output and when doing 24,384 was 24->384 worst timing: in the three levels 24,192,384 was tried first.
now since 384->output is the worst timing, the timing can't be improved since there is no other division of 768 that is greater than 384.
To further optimize also 16,192,384 and 16,128,384 were tested to observe further area improvements, but both of them had worse timing than the initial 24,192,384 so that was chosen finally

// level 3 explo
				Level 1	Level 2	Level 3	Worst timing
3	6095.39	8973	1.08E-03	24	192	384	384->output
	NA	NA	NA	16	192	384	NA
3	7108.318	8979	1.40E-03	16	128	384	16->128






\subsubsection{Majority classifier}
\label{ref:lfsr_opt}

\todo{Majority Classifier non lfsr}
  \todo{subsection area}
  \todo{subsection power}
  \todo{subsection timing}
\todo{Majority Classifier with lfsr optimization}
  \todo{subsection area}
  \todo{subsection power}
  \todo{subsection timing}






\section{Discussion}
\label{sec:discussion}
\todo{timing is slower in lower hidden size}

%   - [ ] accuracy differences were not that great and partially due to random chance
	% - [ ] Hyperparameter tuning might be overfitting on the testset
    \subsection{Accuracy Differences and Hyperparameter Tuning}
    The observed differences in accuracy were not substantial and might be attributed to random variations rather than significant improvements or degradations in performance. Additionally, there is a possibility that hyperparameter tuning may have led to overfitting on the test set. This means that while the model performs well on the specific test data, it may not generalize as effectively to new, unseen data.
    % - [ ] Rationale
    \subsection{Rationale for Design Decisions}
    %   - [ ] somewhere needs to be put a better reasoning why 2 matrices are more efficient: either have 1024 counters = excessive or 1024x768 counts which is slow and area gains are low
    \begin{itemize}
        \item Efficiency of Two Matrices
        Using two matrices is more efficient for several reasons. Employing 1024 counters is considered excessive in terms of hardware resources. Additionally, utilizing 1024x768 counts can be slow and offers minimal area gains, making it an inefficient choice.
         % 	- [ ] why not CNN decision: would probably require somewhere a linear layer anyways, or a large number of layers vastly reducing speed
        \item Decision Against Using CNN
        Implementing a CNN would likely require a linear layer at some stage or a substantial number of layers, which would significantly reduce speed. The potential complexity and depth of CNNs would lead to a decrease in operational speed, making them less suitable for the intended application.
         % 	- [ ] Small section that weight memory is most of the area
        \item Impact of Weight Memory
        Weight memory constitutes a significant portion of the total area. Managing and optimizing weight memory is crucial to maintaining an efficient design.
    \end{itemize}
	%     - [ ] section on hardware possible strategies given Fully connected bnn
    \subsection{Hardware Possible Strategies for Fully Connected BNN}
    \begin{itemize}
        % 	    - [ ] subsubsection strategy do everything sequentially
        \item Sequential Strategy
        Performing all operations sequentially is a simpler approach but might be unnecessarily slow. It offers minimal area and power savings due to the mandatory weight storage requirements.
        % 	    - [ ] subsubsection strategy do everything in parallel in one tick
        \item Parallel Strategy
        Executing all operations in parallel within a single clock tick can be extremely fast. However, this method requires a large amount of hardware resources, making it impractical for many applications due to its substantial area and power demands.
        % 	    - [ ] subsubsection strategy mixed approach, since parallel would be huge and sequentially would be unneccesary slow for little area and power gains due to weights storage that is mandatory
        \item Mixed Strategy
        The hybrid approach to processing combines elements of both sequential and parallel processing strategies. By blending these methods, it seeks to leverage the strengths of each while mitigating their respective drawbacks. This approach aims to strike a balance between speed and resource usage, optimizing performance without excessively draining area and power. It aims to avoid the inefficiencies inherent in purely sequential processing, such as long execution times, while also sidestepping the challenges of purely parallel processing, such as increased complexity and potential bottlenecks. Ultimately, the hybrid approach offers a compromise solution that aims to deliver efficient and effective processing capabilities across various applications and domains.
    \end{itemize}
\section{Conclusions}
\label{sec:conclusions}
	- [ ] exploration of multiple hidden sizes was very benificial
	- [ ] timing bottleneck mostly from memory


\todo{future work}
	- [done ] subsection better memory library
	- [done ] subsection also try 729 as input size for xnor popcount csa network

\section{future work}
\label{sec:future}
	\subsection{Better Memory Library}
Improving the memory library is an important step for future implementations of the BNN. In this paper, the memory components in our design are based on standard libraries, which may not be optimized for the specific needs of BNN operations. By developing or integrating a specialized memory library specifically for binary operations, we can significantly reduce the access time and energy consumption. This specialized memory can include optimized SRAM or emerging memory technologies like ReRAM, which are more suited for the frequent read-write cycles in BNN computations. A better memory library will contribute to lower latency and higher efficiency, ultimately improving the overall performance of the BNN hardware.

	\subsection{Input Size Optimization with 729}
Another promising area for future work is experimenting with an input size of 729 for the XNOR-popcount CSA network. This might enhance the alignment and efficiency of the XNOR and popcount operations. This approach could lead to more balanced computational loads and possibly reduce the overall hardware complexity. Testing this input size may uncover further optimizations in terms of area and power without compromising accuracy.

\printbibliography

\newpage


\appendix
\label{appendix}

\section{Mathematical Foundations for Hardware Implementations}

\label{appendix:bnn_maths}

\subsection{First matrix multiplication with activation}

This section demonstrates that performing matrix-vector multiplication followed by a hard activation is equivalent to using XNOR operations, popcount, and a subsequent comparison.
The matrix-vector multiplication \(\mathbf{A} \mathbf{b}\) shown in Equation \ref{eq:matrix_vector_multiplication} results in a vector \(\mathbf{c}\). Each element \(c_i\) of this vector is obtained by the dot product of the \(i\)-th row of matrix \(\mathbf{A}\) with vector \(\mathbf{b}\), as described in Equation \ref{eq:dot_product}. This can be further expanded into component-wise multiplication and summation (Equation \ref{eq:component_wise_dot}). When elements \(a_{ij}\) and \(b_j\) are binary (\(1\) or \(-1\)), we map them to \(1\) and \(0\) respectively, as shown in Equation \ref{eq:mapping}, enabling the use of XNOR for the multiplication, as described in Equation \ref{eq:xnor}.

\begin{figure}[ht]
    \centering
    \begin{equation}
    \mathbf{c} = \mathbf{A} \mathbf{b}
    \label{eq:matrix_vector_multiplication}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n a_{ij} b_j
    \label{eq:dot_product}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n (a_{ij} \cdot b_j)
    \label{eq:component_wise_dot}
    \end{equation}

    \begin{equation}
    a'_{ij} = \begin{cases}
    1 & \text{if } a_{ij} = 1 \\
    0 & \text{if } a_{ij} = -1
    \end{cases}, \quad
    b'_j = \begin{cases}
    1 & \text{if } b_j = 1 \\
    0 & \text{if } b_j = -1
    \end{cases}
    \label{eq:mapping}
    \end{equation}

    \begin{equation}
    a_{ij} \cdot b_j = \text{XNOR}(a'_{ij}, b'_j)
    \label{eq:xnor}
    \end{equation}

    \caption{Matrix-vector multiplication and equivalent dot product representation.}
    \label{fig:matrix_vector_multiplication}
\end{figure}


\begin{figure}[ht]
    \centering
    \begin{equation}
    c = \sum_{i=1}^n a_i b_i
    \label{eq:dot_product_sum}
    \end{equation}

    \begin{equation}
    c = \left|\{c_i \mid c_i = 1\}\right| - \left|\{c_i \mid c_i = -1\}\right|
    \label{eq:dot_product_count}
    \end{equation}

    \begin{equation}
    |\{c_i \mid c_i = -1\}| = n - |\{c_i \mid c_i = 1\}|
    \label{eq:negative_count}
    \end{equation}

    \begin{equation}
    a \cdot b = 2 \left|\{c_i \mid c_i = 1\}\right| - n
    \label{eq:dot_product_simplified}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } 2 \left|\{c_i \mid c_i = 1\}\right| \geq n \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } \left|\{c_i \mid c_i = 1\}\right| \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function_half}
    \end{equation}

    \begin{equation}
    \text{activation}(a \text{ XNOR } b) = \begin{cases}
    1 & \text{if } \text{popcount}(a \text{ XNOR } b) \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_xnor}
    \end{equation}

    \caption{Dot product calculation and activation function.}
    \label{fig:dot_product_activation}
\end{figure}

In Figure \ref{fig:dot_product_activation}, Equation \ref{eq:dot_product_sum} sums the products of the components. Equation \ref{eq:dot_product_count} translates the sum into a count of \(1\)s and \(-1\)s. The total count of \(-1\)s can be expressed as the complement of the count of \(1\)s (Equation \ref{eq:negative_count}). The dot product simplifies to Equation \ref{eq:dot_product_simplified}. The activation function (Equation \ref{eq:activation_function}) sets the output based on the threshold condition, which can also be expressed equivalently as shown in Equation \ref{eq:activation_function_half}. This is related to the XNOR operation in Equation \ref{eq:activation_xnor}, where the activation depends on the population count of the XNOR operation being greater than or equal to half the length of the vector.


\subsection{maths second matrix}
\todo{subsubsection maths is same but now the actual highest popcount needs to be found}




\vspace{12pt}
\showtodocount


\end{document}


