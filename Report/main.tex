
 % -- just a two column document
\documentclass[conference]{IEEEtran}
% \documentclass[twocolumn]{tudelft-aiaa}

\usepackage[
backend=biber,
style=ieee
]{biblatex}
\addbibresource{ref.bib}

% \usepackage{todonotes}
% \usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{ifthen}
\usepackage{etoolbox}
\usepackage{hyperref}
% \maxdeadcycles=200
\usepackage{tocloft}

 \usepackage{booktabs}

% Package for better math typesetting
\usepackage{amsmath}

% Package for custom lists
\usepackage{enumitem}



% Package to count todos

% Counter for todos
\newcounter{todocount}
\setcounter{todocount}{0}

% \todo command
\newcommand{\todo}[1]{
  \stepcounter{todocount}
}

% Display todo count
\newcommand{\showtodocount}{%
  \ifthenelse{\value{todocount}=0}{%
    % No todos
    \section*{Todo List}
    No todos.
  }{%
    % Todos found
    \section*{Todo List}
    Total todos: \arabic{todocount}.
  }
}







\title{Hardware Implementation of MNIST digit recognizer using Binary Neural Networks}





\author{
\IEEEauthorblockN{Joshua Azimullah}
\IEEEauthorblockA{5054354\\
j.r.azimullah@tudelft.nl}
\and
\IEEEauthorblockN{Pieter Becking}
\IEEEauthorblockA{4685377\\
pbecking@tudelft.nl}
\and
\IEEEauthorblockN{Christian van den Berg}
\IEEEauthorblockA{5401674\\
c.vandenberg-1@tudelft.nl}
\and
\IEEEauthorblockN{Ioannis Karydis}
\IEEEauthorblockA{5954460\\
ikarydis@tudelft.nl}
}

\begin{document}

\maketitle


\begin{abstract}
\end{abstract}

\todo{general todos}
\todo{work out all todos}
\todo{Each section in the beginning describes what it will say}
\todo{Connecting signal words?}

\section{Introduction}
\label{sec:introduction}

\subsection{Basic Theory of Binary Neural Networks}

Binary Neural Networks (BNNs) reduce all computational values to 1-bit, representing them as either \(+1\) or \(-1\). This quantization allows for the replacement of traditional Multiply-Accumulate (MAC) operations with XNOR and popcount operations, significantly enhancing computational speed and reducing hardware complexity and power consumption \cite{courbariaux2016binarynet, simons2019review}. The detailed mathematical framework is provided in Appendix \ref{appendix:bnn_maths}.

\subsection{Scope of the Project}

The objective of this project is to develop a BNN device utilizing the Genus \cite{genus} tool and the Nangate Open Cell Library \cite{nangate_lib} to classify MNIST test set images with at least 95\% accuracy. Upon achieving this accuracy, the focus shifts to optimizing the device to minimize timing, area, and power requirements. These optimizations aim to decrease the number of clock cycles, thereby reducing energy consumption and overall area, thus lowering power consumption.

\section{Outline}
\label{sec:outline}

Section \ref{sec:overview} provides an overview of the BNN architecture. Section \ref{sec:implementation} details the software and hardware implementation strategies used for the BNN, including training methodologies and tooling for device synthesis. Section \ref{sec:results} presents the results, including simulation setups, demonstration outputs, and optimization explorations. Moreover, Section \ref{sec:discussion} discusses the design choices and observed variabilities, and Section \ref{sec:conclusions} concludes with the success of the project and potential areas for future work, as expanded in Section \ref{sec:future}. The appendix contains any additional mathematical foundations for the hardware implementations.


\section{Overview of BNN Architecture}
\label{sec:overview}

Section \ref{ref:why_768} justifies the selection of 768 input values instead of 784, demonstrating enhanced performance in this context.

The BNN architecture is fully connected, consisting of an input layer with 768 nodes, a hidden layer initially with 1024 nodes (optimized to 608 nodes), and an output layer with 10 nodes for digit classification. During training, the hidden layer uses a hardtanh activation, while the inference phase employs a hard activation function to produce outputs strictly as \(-1\) or \(+1\). The highest sum from the output layer determines the predicted digit.


\section{Implementation of BNN architecture}
\label{sec:implementation}
\subsection{Software implementation}
\todo{list learning rate, batch size}

Our BNNs are trained using PyTorch\cite{paszke2019pytorch}, leveraging the Adam optimizer\cite{kingma2014adam} and CrossEntropy loss function\cite{mao2023crossentropy}, with one-hot encoded outputs. Prior to training, the input values are rounded to \(\{-1, 1\}\), ensuring the network adapts to these binary inputs. Training is conducted in floating-point representation, with binarization applied post-training.

In the MNIST dataset, the 8-bit input values are pre-processed such that values are mapped to $1$ if $\geq 128$, and to $-1$ otherwise. The HardTanh function is used during training to nudge activations towards hard binary values. like in paper: \cite{yuan2023comprehensive}

Parallel evaluations using NumPy, simulating the potential BNN's performance, involve rounding matrices to \(\{-1, 1\}\) to validate that inference based on the matrix multiplications using the rounded values aligns with the outcomes predicted by the PyTorch model.





\subsection{Hardware Tooling}

The device was synthesized using the Genus tool \cite{genus} with the Nangate Open Cell Library \cite{nangate_lib}, and implemented in VHDL. Testing simulations were conducted with GHDL \cite{ghdl}, which enabled efficient simulation and expedited testing iterations.


\subsection{Hardware implementation}
\autoref{fig:overview} shows the hardware architecture of the Binary Neural Network (BNN). 

The XNOR Popcount Evaluator (\autoref{fig:xnor_popcount}) performs the first matrix multiplication and activation by XNORing and popcounting the input vector with the current row of weights. It outputs a bit indicating if the sum is $\geq 384$. The Majority Classifier (\autoref{fig:majority_classifier}) conducts the second matrix multiplication, determining the output by choosing the highest sum among the columns.

In each clock cycle, a row from the first matrix (XNOR Popcount) and a column from the second matrix (Majority Classifier) are processed in parallel. The XNOR Popcount Evaluator is the bottleneck, so the second matrix operation isn't accelerated. 

On reset, both matrix\_1 and matrix\_2 counters reset to 0. These counters, as shown in \autoref{fig:overview}, select the current row and column in the respective matrix register files. The matrix\_1 counter increments each cycle until it completes the register file. 

The XNOR Popcount Evaluator takes the input vector and the current row from weights\_1, XNORs them, and popcounts the result, outputting a bit if the sum $\geq 384$. It also produces a validity bit, initially set low, which counts to a specific count $n$ using an LFSR counter, after which it is set high. This bit is set after $n$ cycles based on the optimization in Section \ref{ref:pipeline_popcount}.

The Majority Classifier activates if the validity bit is set. Upon activation, it increments the matrix\_2 counter after each cycle, resulting in a delay behind the matrix\_1 counter by $n$. No additional enable signal is needed; a reset to 0 starts the prediction process directly. The Majority Classifier also holds a count equivalent to the matrix\_2 count, and when this count equals the hidden size $1024$, the done flag is set high, signaling prediction completion.




\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{overview.pdf}
    \caption{Overview of the hardware architecture.}
    \label{fig:overview}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{Xnor_popcount.pdf}
    \caption{XNOR Popcount Evaluator.}
    \label{fig:xnor_popcount}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{majority_classifier.pdf}
    \caption{Majority Classifier.}
    \label{fig:majority_classifier}
\end{figure}



\subsection{Optimizations}
\subsubsection{Pipelining Popcount}
\hfill\\
\label{ref:pipeline_popcount}

Instead of performing the popcount in a single clock cycle using a large Carry-Save Adder (CSA) network, our exploration in Section \ref{ref:xnor_popcount} determined that a pipelined approach with four registers between the adder stages is optimal. Thus, the popcount result becomes valid after $n=4$ cycles, with subsequent cycles producing new valid sums.

\subsubsection{LFSR-Based Majority Classifier}
\hfill\\
\label{ref:lfsr_opt}

Initially, individual counters for each digit were used, along with a tree structure to identify the highest count. However, this structure is redundant if the highest count is tracked dynamically. Each counter is compared against the current highest count. If a counter matches the highest count and is to be incremented, the highest count is updated accordingly. This update is achieved by performing a bitwise AND operation between a 10-bit vector indicating counters equal to the highest count and a 10-bit vector indicating counters to be incremented. An OR reduction of this resultant vector determines if the highest count should increase. 
Ultimately, this approach allows the additional optimization of replacing normal counters with Linear Feedback Shift Register (LFSR) counters, which provide equivalent functionality using less area \cite{xilinx_lfsr}, as is shown in Section \ref{ref:hardware_metrics}

\subsubsection{Hidden Layer Size Optimization}
\hfill\\

The hidden layer size was optimized by reducing it from its initial configuration, as discussed extensively in Section \ref{ref:hidden_size_optimization}.


\section{Results}
\label{sec:results}
\subsection{Simulation Setup}

The initial model, developed in PyTorch, was translated into raw NumPy operations using values of 1 and -1. Upon validation, the NumPy implementation was segmented into the XNOR Popcount and Majority Classifier components.

This segmentation allowed the generation of random test data for both components, output as text files containing 0s and 1s. These text files served as inputs for testbenches to validate the functionality of each component individually.

Additionally, text files for input vectors and corresponding labels were generated using the same method. This enabled testing the entire 10,000-image MNIST \cite{mnist} testset to verify the device's operation.




\subsection{Demonstration}
This section provides a detailed demonstration of the simulation results. 
\autoref{fig:waveform} presents a waveform analysis demonstrating that, after the reset signal is set low, the output vector begins displaying which counters are equal to the highest count. When the done signal is high, the output equals to the expected signal, verifying the proper operation of the designed circuit.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.2\textwidth]{./testbench.png}
    \caption{Testbench output validating the images from the MNIST test set \cite{mnist}.}
    \label{fig:testbench}
\end{figure}

\autoref{fig:testbench} displays the output of the testbench, which evaluates the entire set of 10,000 images against the full MNIST test set, confirming the functionality and accuracy of the simulation.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{./Waveform_picture.png}
    \caption{Waveform illustrating the comparison against the output\_row and expected\_row during the simulation of the testbench}
    \label{fig:waveform}
\end{figure}





\subsection{Hardware Metric Results}
\label{ref:hardware_metrics}

This section presents the results achieved in terms of timing, area, power, and accuracy. Among these, only the first optimization impacts accuracy; the latter two affect timing, area, and power metrics exclusively. A comparison of these optimizations for the total metrics is summarized in Table \ref{tab:optimization_metrics}. 


\begin{table}[h]
    \centering
    \caption{Comparison of Hardware Metrics Across Different Optimizations, including inference time}
    \label{tab:optimization_metrics}
    \begin{tabular}{@{}ccccccccc@{}}
        \toprule
        \textbf{Optimization} & \textbf{Area ($\mu m^2$)} & \textbf{Energy (nJ)} & \textbf{Accuracy (\%)} & \textbf{Inference (ns)}  \\
        \midrule
        Original & 113127 & 45.87 & 95.07 & 3059 \\
        Hidden Size & 68377 & 22.18 & 95.65 & 2007 \\
        Level & 73059 & 20.18 & 95.65 & 1739 \\
        LFSR & 70088 & 18.57 & 95.65 & 1639 \\
        \bottomrule
    \end{tabular}
\end{table}

\autoref{tab:hardware_metrics} illustrates that the bulk of the area and power consumption is attributed to the weight storage in the register files. It is evident that the XNOR popcount optimization, detailed in Section \ref{ref:xnor_popcount}, results in increased area and power but enhances the timing slack. Conversely, the majority classifier optimization, as discussed in Section \ref{ref:lfsr_opt}, improves area, power, and timing slack across all metrics. Similarly, optimizing the hidden size optimizes the register files, thereby reducing timing slack, although it also incurs increases in area and power.

\begin{table}[h]
    \centering
    \caption{Hardware Metrics for Device Parts: Impact of Optimizations}
    \label{tab:hardware_metrics}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Device Part} & \textbf{Area (\(\mu m^2\))} & \textbf{Power (W)} & \textbf{Time Slack (ps)} \\
        \midrule
        Register Files Old & 104334 & 1.36E-02 & 7016 \\
        Register Files New & 62653 & 1.00E-02 & 6704 \\
        XNOR Popcount Old & 4487 & 7.30E-04 & 7957 \\
        XNOR Popcount New & 6095 & 1.08E-03 & 8973 \\
        Majority Classifier Old & 4307 & 6.86E-04 & 8825 \\
        Majority Classifier New & 1340 & 2.38E-04 & 9046 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Power consumption}
\todo{section calculation of power}
\todo{number of cycles = hidden size + delay from xnor popcount}
\todo{power from number of cycles and watts}

\subsubsection{Note on Input Size}
\hfill\\
\label{ref:why_768}

During the design phase, the input size was set to 768 instead of 784 to align with hardware efficiency, as 768 (\(2^9 + 2^8\)) simplifies binary implementation. The 28x28 input image was truncated to 768 inputs by excluding the last 16 pixels. Empirical testing confirmed no accuracy impact, hence, 784-input designs were not pursued.


\subsection{Hidden Size Optimization}
\label{ref:hidden_size_optimization}

Given that our Binary Neural Network (BNN) device employs a sequential approach for processing the hidden layer, it is crucial to finely tune the hidden size to balance accuracy and hardware efficiency. Increasing the hidden size directly correlates with an increase in the number of clock cycles required for inference.
Since the hidden size affects both the first and second weight register files proportionally, any reduction in hidden size guarantees substantial gains across all hardware metrics except accuracy.

An initial broad exploration was conducted using hidden sizes that are powers of two: 64, 128, 256, 512, 1024, 2048, 4096. From this analysis, it was determined that a hidden size of 1024 was sufficient to achieve the target accuracy of 95\%, as illustrated in \autoref{fig:line_graph_global}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{Global_exploration_hidden_sizes.png}
    \caption{Global exploration of hidden sizes and their impact on accuracy.}
    \label{fig:line_graph_global}
\end{figure}

For further optimization, the hidden size was refined in increments of 32 between 512 and 1024. The maximum achieved accuracy for each hidden size within this range is depicted in \autoref{fig:bar_max_achieved_accuracy}. Through this fine-grained exploration, a hidden size of 608 was identified as optimal, reaching the target accuracy of 95\% with a slight improvement to 95.65\%.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{Different_hidden_layer_sizes_512_1024.png}
    \caption{Maximum achieved accuracy for different hidden layer sizes between 512 and 1024.}
    \label{fig:bar_max_achieved_accuracy}
\end{figure}


\subsection{XNOR popcount Optimization}
\label{ref:xnor_popcount}

This section explores optimizing the XNOR popcount module with a focus on reducing timing slack to enhance clock speed. Given that the majority of area and power consumption arises from the register files, as indicated in \ref{ref:hardware_metrics}, the strategy centered on minimizing slack to improve performance efficiency.

\subsubsection{Initial Approach: Direct CSA Tree Implementation}
\hfill\\

The initial exploration of the XNOR popcount involved synthesizing a direct Carry-Save Adder (CSA) tree to provide a foundational understanding of the basic performance metrics. This method utilizes a full direct CSA tree to compute the population count. Only the XNOR popcount part was synthesized, which facilitated a more rapid assessment. The results for this direct CSA implementation, including area, timing, and power, are summarized in Table \ref{tab:direct_csa}. This direct implementation serves as a reference point before introducing pipelining for performance improvements.


\begin{table}[h]
    \centering
    \caption{Metrics for Direct CSA Tree Implementation}
    \label{tab:direct_csa}
    \begin{tabular}{@{}ccc@{}}
        \toprule
        \textbf{Area ($\mu m^2$)} & \textbf{Timing (ps)} & \textbf{Power (W)} \\
        \midrule
          4487 & 7957 & 7.30E-04 \\
        \bottomrule
    \end{tabular}
\end{table}


\subsubsection{First-Level Pipelining Exploration}
\hfill\\

To optimize performance and reduce the required minimum slack, pipelining stages were introduced within the CSA network. This exploration started by examining a single intermediary level of sum registers. These registers accumulate counts up to a specified maximum before passing their sum to the next stage. For example, a configuration with a maximum sum of 64 means that the CSA tree computes partial sums of the XNOR output vector in chunks of 64, leading to intermediate registers each counting up to 64. These intermediate sums are then aggregated by another CSA tree to produce the final output sum. Various configurations were explored, where the maximum sums ranged from 16 to 384. Table \ref{tab:first_level} presents the area, timing, and power metrics for these configurations.

\todo{this NA}

\begin{table}[h]
    \centering
    \caption{Metrics for CSA Tree with One Level of Pipelining}
    \label{tab:first_level}
    \begin{tabular}{@{}ccccc@{}}
        \toprule
        \textbf{Max Sum} & \textbf{Area ($\mu m^2$)} & \textbf{Timing (ps)} & \textbf{Power (W)} \\
        \midrule
        16 & 6664.63 & 8318 & 1.30E-03 \\
        32 & 5739.482 & 8458 & 1.06E-03 \\
        48 & 5298.986 & 8604 & 9.40E-04 \\
        64 & 5241.796 & 8618 & 9.58E-04 \\
        96 & 4986.702 & 8528 & 8.79E-04 \\
        128 & 4937.758 & 8457 & 8.47E-04 \\
        192 & 4793.054 & 8251 & 8.41E-04 \\
        256 & NA & NA & NA \\
        384 & NA & NA & NA \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Two-Level Pipelining Exploration}
\hfill\\

To further refine the pipelining strategy, a two-level configuration was tested. Due to the extensive nature of possible combinations, a targeted exploration approach was used. This began with the insertion of an additional level of sum registers, capable of holding up to 256 counts, between the initial sum of 64 and the final output sum from the best performing configuration in Table \ref{tab:first_level}. Each test subsequently modified the maximum sum at the stage exhibiting the worst timing in the previous test. For example, in the configuration where Level 1 had a maximum sum of 64 and Level 2 had 256, the worst timing was observed between the input and the 64-bit stage. This led to the next test where the Level 1 sum was reduced to 16. The configurations explored and their results are detailed in Table \ref{tab:two_level}. This iterative approach aimed to minimize the worst timing while considering area and power metrics.


\begin{table}[h]
    \centering
    \caption{Metrics for CSA Tree with Two Levels of Pipelining}
    \label{tab:two_level}
    \begin{tabular}{@{}cccccc@{}}
        \toprule
        \textbf{Levels} & \textbf{Area ($\mu m^2$)} & \textbf{Timing (ps)} & \textbf{Power (W)} & \textbf{Worst} \\
        \midrule
        64, 256 & 5365 & 8617 & 1.01E-03 & input $\rightarrow$ 64 \\
        16, 256 & 6865 & 8740 & 1.36E-03 & 16 $\rightarrow$ 256 \\
        48, 192 & 5526 & 8750 & 9.84E-04 & input $\rightarrow$ 48 \\
        32, 192 & 5960 & 8834 & 1.11E-03 & input $\rightarrow$ 32 \\
        24, 192 & 5999 & 8851 & 1.07E-03 & 192 $\rightarrow$ out\\
        24, 384 & 5900 & 8740 & 1.05E-03 & 24 $\rightarrow$ 384 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Three-Level Pipelining Exploration}
\hfill\\

Finally, a three-level pipelining configuration was investigated to achieve further optimization. Starting with the best-performing two-level configuration (24, 192), a third level was added with the maximum sum set to 384. The rationale was to mitigate the worst timing observed at the 192 to output transition by adding another intermediary stage. Subsequent configurations tested included 24, 192, 384, and 16, 128, 384 to observe the effects on area and timing. The configuration of levels at 24, 192, 384 demonstrated the best balance across area, timing, and power, making it the chosen configuration. 

Introducing an additional layer does not yield better timing performance, as the critical timing slack occurs between the 384 sum level and the output. Given that 768 cannot be divided into larger segments than 384, further division does not mitigate this bottleneck. 
The results for the three-level pipelining exploration are provided in Table \ref{tab:three_level}.

\todo{this NA}
\begin{table}[h]
    \centering
    \caption{Metrics for CSA Tree with Three Levels of Pipelining}
    \label{tab:three_level}
    \begin{tabular}{@{}ccccccc@{}}
        \toprule
        \textbf{Levels} & \textbf{Area ($\mu m^2$)} & \textbf{Timing (ps)} & \textbf{Power (W)} & \textbf{Worst} \\
        \midrule
        24, 192, 384 & 6095 & 8973 & 1.08E-03 & 384 $\rightarrow$ out\\
        16, 192, 384 & NA & NA & NA & NA \\
        16, 128, 384 & 7108 & 8979 & 1.40E-03 & 16 $\rightarrow$ 128 \\
        \bottomrule
    \end{tabular}
\end{table}

\subsubsection{Conclusion XNOR Popcount exploration}
\hfill\\

Through methodical exploration of various pipelining levels, the implementation of intermediary sum registers proved effective in enhancing the XNOR popcount performance. The three-level configuration with maximum sums of 24, 192, and 384 provided the most optimal trade-off between area, timing, and power, as indicated by the iterative testing and metrics obtained.





\section{Discussion}
\label{sec:discussion}

\subsection{Decision for Fully Connected Network}

The decision to use a fully connected network instead of a Convolutional Neural Network (CNN) was based on initial considerations of complexity. A CNN was expected to require significantly more layers, leading to potentially higher computational demands. However, post-design analysis, as presented in Section \ref{ref:hardware_metrics}, revealed that the bulk of area, power, and timing costs originated from weight memory. Given this, a CNN could potentially offer greater efficiency despite its larger number of layers and hidden sizes due to more effective weight sharing and reduced memory footprint.

\subsection{Hardware Implementation Strategies}

Hardware implementations range from highly sequential designs, minimizing area at the cost of increased cycles, to fully parallel designs, which minimize cycle count but require extensive hardware. Initial attempts to implement a single matrix model failed to achieve the desired accuracy. This approach would necessitate either a large parallel summing network for each row's popcount or a sequential process for each row, both of which were impractical. Opting for a two-matrix model was optimal, achieving our accuracy target of 95\% while balancing hardware complexity and performance.

\subsection{Randomness in Genus Synthesis}

Significant variability was observed in the synthesis process using the Genus \cite{genus} tool, particularly when incorporating weights. Identical hardware code with different weights resulted in performance variations of up to 4.5\%, which likely contributed to the observed timing performance discrepancies when reducing the hidden size from 1024 to 608. Lastly, the hidden size optimization (Section \ref{ref:hidden_size_optimization}) may have inadvertently fit the test conditions, and further steps should be taken to mitigate this issue in future analyses.


\section{Conclusions}
\label{sec:conclusions}

Our aim of achieving 95\% accuracy was successful. 
\todo{this XX percent}
The exploration of various hidden sizes was crucial in enhancing accuracy and optimizing hardware performance. The weight storage was identified as the primary bottleneck, particularly due to register file demands. Our optimizations resulted in a 2x performance increase, a 38\% reduction in design area, and a 59.5\% decrease in energy per inference.


\section{Future work}
\label{sec:future}

Future work includes exploring the integration of alternative SRAM libraries, such as OpenRAM \cite{openram}, to potentially enhance the efficiency of the memory subsystem. Currently everything is implemented within the NandGateOpenCellLibrary \cite{nangate_lib}. The incorporation of a different SRAM solution may offer advantages in terms of power consumption, area utilization, and access times.

Additionally, there is potential to experiment with using 729 as the input size for the XNOR popcount CSA network, dropping even more weights than the optimization of Section \ref{ref:why_768}. This specific input size is optimal for Wallace tree structures, which could improve tree load balancing and computational efficiency \cite{wallace}. Adapting the input size to align with the characteristics of a Wallace tree might enhance the overall performance and timing of the CSA network.

\printbibliography


\appendix
\label{appendix}

\section{Mathematical Foundations for Hardware Implementations}

\label{appendix:bnn_maths}

\subsection{First matrix multiplication with activation}

This section demonstrates that performing matrix-vector multiplication followed by a hard activation is equivalent to using XNOR operations, popcount, and a subsequent comparison.
The matrix-vector multiplication \(\mathbf{A} \mathbf{b}\) shown in Equation \ref{eq:matrix_vector_multiplication} results in a vector \(\mathbf{c}\). Each element \(c_i\) of this vector is obtained by the dot product of the \(i\)-th row of matrix \(\mathbf{A}\) with vector \(\mathbf{b}\), as described in Equation \ref{eq:dot_product}. This can be further expanded into component-wise multiplication and summation (Equation \ref{eq:component_wise_dot}). When elements \(a_{ij}\) and \(b_j\) are binary (\(1\) or \(-1\)), we map them to \(1\) and \(0\) respectively, as shown in Equation \ref{eq:mapping}, enabling the use of XNOR for the multiplication, as described in Equation \ref{eq:xnor}.

\begin{figure}[t]
    \centering
    \begin{equation}
    \mathbf{c} = \mathbf{A} \mathbf{b}
    \label{eq:matrix_vector_multiplication}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n a_{ij} b_j
    \label{eq:dot_product}
    \end{equation}

    \begin{equation}
    c_i = \sum_{j=1}^n (a_{ij} \cdot b_j)
    \label{eq:component_wise_dot}
    \end{equation}

    \begin{equation}
    a'_{ij} = \begin{cases}
    1 & \text{if } a_{ij} = 1 \\
    0 & \text{if } a_{ij} = -1
    \end{cases}, \quad
    b'_j = \begin{cases}
    1 & \text{if } b_j = 1 \\
    0 & \text{if } b_j = -1
    \end{cases}
    \label{eq:mapping}
    \end{equation}

    \begin{equation}
    a_{ij} \cdot b_j = \text{XNOR}(a'_{ij}, b'_j)
    \label{eq:xnor}
    \end{equation}

    \caption{Matrix-vector multiplication and equivalent dot product representation.}
    \label{fig:matrix_vector_multiplication}
% \end{figure}


% \begin{figure}[ht]
    \centering
    \begin{equation}
    c = \sum_{i=1}^n a_i b_i
    \label{eq:dot_product_sum}
    \end{equation}

    \begin{equation}
    c = \left|\{c_i \mid c_i = 1\}\right| - \left|\{c_i \mid c_i = -1\}\right|
    \label{eq:dot_product_count}
    \end{equation}

    \begin{equation}
    |\{c_i \mid c_i = -1\}| = n - |\{c_i \mid c_i = 1\}|
    \label{eq:negative_count}
    \end{equation}

    \begin{equation}
    a \cdot b = 2 \left|\{c_i \mid c_i = 1\}\right| - n
    \label{eq:dot_product_simplified}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } 2 \left|\{c_i \mid c_i = 1\}\right| \geq n \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function}
    \end{equation}

    \begin{equation}
    \text{activation}(c) = \begin{cases}
    1 & \text{if } \left|\{c_i \mid c_i = 1\}\right| \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_function_half}
    \end{equation}

    \begin{equation}
    \text{activation}(a \text{ XNOR } b) = \begin{cases}
    1 & \text{if } \text{popcount}(a \text{ XNOR } b) \geq \frac{n}{2} \\
    -1 & \text{otherwise}
    \end{cases}
    \label{eq:activation_xnor}
    \end{equation}

    \caption{Dot product calculation and activation function.}
    \label{fig:dot_product_activation}
\end{figure}

In Figure \ref{fig:dot_product_activation}, Equation \ref{eq:dot_product_sum} sums the products of the components. Equation \ref{eq:dot_product_count} translates the sum into a count of \(1\)s and \(-1\)s. The total count of \(-1\)s can be expressed as the complement of the count of \(1\)s (Equation \ref{eq:negative_count}). The dot product simplifies to Equation \ref{eq:dot_product_simplified}. The activation function (Equation \ref{eq:activation_function}) sets the output based on the threshold condition, which can also be expressed equivalently as shown in Equation \ref{eq:activation_function_half}. This is related to the XNOR operation in Equation \ref{eq:activation_xnor}, where the activation depends on the population count of the XNOR operation being greater than or equal to half the length of the vector.


\subsection{Second Matrix Multiplication with Maximum Popcount}

The mathematical approach for the second matrix multiplication mirrors the first, involving XNOR operations and popcounts. However, instead of applying an activation function based on a threshold, the objective is to determine the highest popcount across the resulting vectors. The row with the highest popcount equals the prediction made by the model.





\vspace{12pt}
\showtodocount


\end{document}



